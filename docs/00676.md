# Bigdata 了解 Hadoop 及其生态系统

> 原文:[https://devops.com/bigdata-understanding-hadoop-ecosystem/](https://devops.com/bigdata-understanding-hadoop-ecosystem/)

设想我们如何在 DevOps 领域内采用 Hadoop 生态系统是一件非常有趣的事情。我将在接下来的系列文章中介绍它。由 Apache 基金会管理的 Hadoop 是一个用 java 编写的强大的开源平台，能够在使用简单编程模型的计算机集群上以分布式方式大规模处理大量异构数据集。它旨在从单个服务器扩展到数千台机器，每台机器都提供本地计算和存储，并已成为一项受欢迎的技术技能。Hadoop 是一个 Apache 顶级项目，由全球贡献者和用户社区构建和使用。

**Hadoop 架构:**

Apache Hadoop 框架包括以下四个模块:

*   **Hadoop Common:** 包含其他 Hadoop 模块需要的 Java 库和实用程序。这些库提供了文件系统和操作系统级的抽象，包含启动 Hadoop 所需的基本 Java 文件和脚本。
*   **Hadoop 分布式文件系统(HDFS):** 一种分布式文件系统，提供对社区机器上的应用数据的高吞吐量访问，从而在集群中提供非常高的聚合带宽。
*   **Hadoop YARN:** 负责作业调度和集群资源管理的资源管理框架。
*   **Hadoop MapReduce:** 这是一个基于 YARN 的编程模型，用于并行处理大型数据集。

下图描绘了 Hadoop 框架中可用的四个组件。

[![Hadoop](../Images/41c67c3877ad716fd58192c6755c520b.png)T2】](https://devops.com/wp-content/uploads/2015/06/Hadoop.png)

Hadoop 中所有模块的设计都有一个基本假设，即硬件故障，因此应该由框架在软件中自动控制。除了 HDFS、YARN 和 MapReduce，现在人们普遍认为整个 Apache Hadoop“平台”还包括许多相关的项目:Apache Pig、Apache Hive、Apache HBase 等等。

**Hadoop 生态系统:**

Hadoop 因其能够通过商用硬件集群快速、经济高效地存储、分析和访问大量数据而广受欢迎。如果我们说 Apache Hadoop 实际上是几个组件的集合，而不仅仅是一个产品，这并没有错。

对于 Hadoop 生态系统，有几个商业产品和开源产品被广泛用于使 Hadoop 外行人更容易访问和使用。

以下各节提供了有关各个组件的附加信息:

**MapReduce**

Hadoop MapReduce 是一个软件框架，用于轻松编写应用程序，以可靠、容错的方式在大型商用硬件集群上并行处理大量数据。编程方面，MapReduce 中最常见的有**两个函数**。

*   **映射任务:**主计算机或节点获取输入并将其转换成更小的部分，并将其分发到其他工作节点上。所有工作节点解决自己的小问题，并向主节点提供答案。
*   **Reduce 任务:**主节点组合来自工人节点的所有答案，并以某种输出形式形成它，这是我们的大型分布式问题的答案。

通常，输入和输出都保存在文件系统中。该框架负责调度任务，监控它们，甚至重新执行失败的任务。

**Hadoop 分布式文件系统(HDFS)**

HDFS 是一个分布式文件系统，提供对数据的高吞吐量访问。当数据被推送到 HDFS 时，它会自动分割成多个数据块并存储/复制数据，从而确保高可用性和容错能力。

***注*** ***:一个文件由很多块组成(64MB 及以上的大块)。*T9】**

下面是 HDFS 的**主要组成:**

*   **NameNode:** 充当系统的主节点。它维护名称系统，即目录和文件，并管理数据节点上的数据块。
*   **DataNodes:** 它们是部署在每台机器上的从节点，提供实际的存储。它们负责为客户端的读写请求提供服务。
*   **次 NameNode:** 它负责执行定期检查点。如果 NameNode 失败，您可以使用检查点重新启动 NameNode。

**鼠标**

Hive 是 Hadoop 生态系统的一部分，为 Hadoop 提供了一个类似 SQL 的接口。这是一个适用于 Hadoop 的数据仓库系统，可以简化数据汇总、即席查询以及存储在 Hadoop 兼容文件系统中的大型数据集的分析。

它提供了一种将结构投射到数据上的机制，并使用一种类似 SQL 的语言 HiveQL 来查询数据。Hive 还允许传统的 map/reduce 程序员在 HiveQL 中表达这种逻辑不方便或效率低下时插入他们的自定义 map pers 和 reducer。

蜂巢的**主要积木** **是—**

1.  **Metastore**–存储关于列、分区和系统目录的元数据。
2.  **驱动**–管理 HiveQL 语句的生命周期
3.  **查询编译器**–将 HiveQL 编译成有向无环图。
4.  **执行引擎**–按照编译器产生的正确顺序执行任务。
5.  **hive server**–提供节俭接口和 JDBC / ODBC 服务器。

**HBase (Hadoop 数据库)**

HBase 是一个分布式、面向列的数据库，使用 HDFS 作为底层存储。如前所述，HDFS 致力于一次写入多次读取的模式，但这并不总是如此。我们可能需要对大型数据集进行实时读/写随机访问；这就是 HBase 的用武之地。HBase 构建在 HDFS 之上，分布在面向列的数据库上。

下面是 HBase 的**主要** **组件:**

*   **HBase Master:** 它负责协商跨所有区域服务器的负载平衡，并维护集群的状态。它不是实际数据存储或检索路径的一部分。
*   **RegionServer:** 它部署在每台机器上，托管数据并处理 I/O 请求。

动物园管理员

ZooKeeper 是一个集中式服务，用于维护配置信息、命名、提供分布式同步和提供组服务，这些服务对各种分布式系统非常有用。没有 ZooKeeper，HBase 无法运行。

**看象人**

Mahout 是一个可扩展的机器学习库，实现了各种不同的机器学习方法。目前 Mahout 包含四组主要的算法:

*   建议，也称为集体过滤
*   分类，也称为归类
*   使聚集
*   频繁项集挖掘，也称为并行频繁模式挖掘

Mahout 库中的算法属于可以以分布式方式执行的子集，并且已经被编写为可以在 MapReduce 中执行。Mahout 在三个方面是可伸缩的:通过利用算法属性或实现基于 Apache Hadoop 的版本，它可以伸缩到相当大的数据集。

**Sqoop (SQL 到 Hadoop)**

Sqoop 是一个工具，旨在有效地将结构化数据从 SQL Server 和 SQL Azure 传输到 HDFS，然后在 MapReduce 和 Hive 作业中使用它。人们甚至可以使用 Sqoop 将数据从 HDFS 转移到 SQL Server。

**阿帕奇火花:**

Apache Spark 是一个通用计算引擎，提供大规模的快速数据分析。Spark 构建于 HDFS 之上，但绕过了 MapReduce，转而使用自己的数据处理框架。Apache Spark 的常见用例包括实时查询、事件流处理、迭代算法、复杂操作和机器学习。

**猪**

Pig 是一个用于分析和查询巨大数据集的平台，它由一种用于表达数据分析程序的高级语言和用于评估这些程序的基础设施组成。Pig 的内置操作可以理解半结构化数据，如日志文件，并且该语言可以使用 Java 进行扩展，以添加对自定义数据类型和转换的支持。

Pig 有三个主要的**关键属性:**

*   展开性
*   优化机会
*   易于编程

Pig 程序的显著特性是它们的结构易于并行化，这反过来使它们能够处理非常大的数据集。目前，Pig 的基础设施层由一个编译器组成，该编译器生成 MapReduce 程序序列。

**Oozie**

Apache Oozie 是一个管理 Hadoop 作业的工作流/协调系统。

**水槽**

Flume 是一个框架，用于在 Hadoop 中获取、聚合和移动大量日志数据或文本文件。代理遍布于 web 服务器、应用服务器和移动设备内部的 IT 基础设施中。Flume 本身有一个查询处理引擎，所以很容易在将每批新数据传送到目标接收器之前对其进行转换。

安巴里:

Ambari 是为了帮助管理 Hadoop 而创建的。它支持 Hadoop 生态系统中的许多工具，包括 Hive、HBase、Pig、Sqoop 和 Zookeeper。该工具具有一个管理仪表板，可跟踪集群运行状况，并有助于诊断性能问题。

**结论**

Hadoop 功能强大，因为它是可扩展的，并且易于与任何组件集成。它之所以受欢迎，部分是因为它能够跨商用硬件集群快速、经济高效地存储、分析和访问大量数据。Apache Hadoop 实际上不是一个单一的产品，而是几个组件的集合。当所有这些组件合并后，Hadoop 变得非常用户友好。