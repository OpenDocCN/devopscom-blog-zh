# 开发道德人工智能的 4 个关键

> 原文：<https://devops.com/4-keys-to-developing-ethical-ai/>

公司继续努力解决如何以不损害隐私的方式使用人工智能(AI)技术，并考虑其他关键的道德因素。不可否认，AI 是当今发展最快的类别之一。但是，如果我们不能构建符合高道德标准的工具，我们不仅是在伤害最终用户，而且我们也无法兑现这项技术作为一种向善力量的承诺。

几十年来，人工智能和伦理一直是技术界最热门的话题之一。然而，作为一个行业，我们仍然发现很难在确保道德使用方面尽最大努力，更不用说解决这些系统想要解决的问题了。幸运的是，有一个群体可以成为我们需要的指导力量:开发者。

从构思过程到实现，开发人员社区是迄今为止最了解人工智能产品实际工作方式的群体；因此，他们可以对这些工具实际做什么以及它们有多道德产生巨大的影响——他们需要被授权做出这些决定。

考虑到这一点，这里有四个关键问题，这些问题是公司及其开发团队在努力构建未来的道德人工智能解决方案时应该问的。

## 这在各方面都是透明的吗？

如果一个开发人员发现自己在被问及产品如何工作或者产品最终将如何被使用时说，“我不知道”，那么生产应该立即停止。自动化的力量是巨大的。但是考虑到它变形和学习的速度，确保监督和可解释性在它的功能中根深蒂固是确保道德使用的必要条件。如果不能保证这一点，开发人员应该有权标记他们的问题，并在继续生产之前寻找补救方法。

随着人工智能的不断发展，设计和开发人工智能系统可能会继续将我们带入没有明确的法律、社会或政策指南来解决许多潜在用途和滥用的领域。因此，现在是时候让开发者建立一个透明和清晰的沟通渠道，在这里他们可以积极地提出问题，与不同的利益相关者讨论和商议道德难题。这对于减少或消除人工智能的负面影响至关重要。否则，我们将继续看到目前围绕人工智能伦理的失礼行为继续下去。

## 我们是否考虑了风险或超出了监管界限？

事实上，任何伟大的想法都有风险。不幸的是，当涉及到人工智能时，这些风险通常涉及不道德的使用或意想不到的后果。围绕面部识别和人工智能正在进行的辩论就是一个很好的例子。虽然这个用例旨在提高安全性，并且可能有其他好处，但面部识别软件存在重大的隐私风险，并且担心被误用和误用。此外，由于人工智能的监管格局仍在形成中，公司可能会陷入一个陷阱，希望利用这些灰色区域来换取第一个上市或获得竞争优势。这就是为什么没有在面部识别实施中使用道德隐私保护人工智能的公司，例如，不得不收回他们的产品。

简而言之，以这种方式行事会对人工智能产品的道德产生可怕的后果。但是有好消息；纠正这一错误的方法相对简单:不要只是确保产品勉强符合监管灰色地带，而是要确保它们超越这些灰色地带。诚然，这可能导致上市时间增加和大量的跑腿工作。然而，这是现实地“面向未来”的产品对抗潜在的不道德用例的唯一方法。

## 这个系统适应性强吗？

即使是最好的准备和深谋远虑也不能保证工具完全按照预期的方式运行。这就是为什么开发人员有能力为突发事件做计划并在他们的工具中构建适应性是如此重要，以便在出现任何道德问题时可以定期进行调整。当涉及到解决人工智能产品学习和扩展时可能出现的偏见问题时，这可能特别有帮助。

技术是一个动态的、快速发展的范畴。因此，开发人员经常在非常紧凑的时间表上工作。但为了让人工智能正确，开发人员需要时间来找出围绕他们产品的更大图景，并尽可能地确保产品在短期、中期和长期内都是出于道德目的而设计的。如果他们不这样做，不仅会损害公司的声誉，而且公司的整个产品渠道可能会无限期地受阻，因为产品需要完全下线。

## 开发团队真的准备好道德地构建了吗？

对于任何一家科技公司来说，制定一份以道德为中心的具体使命宣言都是一个很好的开始。但是，如果没有一个更系统的方法在开发团队中灌输有原则的行为，公司将很难在追求构建道德产品的过程中取得切实的成果。此外，鉴于道德人工智能开发已经变得如此复杂——这要归功于[量子计算](https://devops.com/how-quantum-computing-can-solve-real-world-problems/)等方面的创新——即使是最有经验的开发人员也很难跟上与公司道德人工智能目标相关的发展和期望标准。

因此，除了发布总体信息，公司还需要通过建立一个完整的基础设施，为开发人员提供持续的培训和资源，使他们能够时刻牢记道德标准，从而将他们的道德意图编纂成文。否则，混乱几乎肯定会接踵而至，失误将会发生，道德标准将会下降。

不幸的是，尽管有这样的承诺，人工智能世界还没有达到我们需要的道德框架，以解决明天的问题，同时也服务于公共利益。但事实并非如此。通过问这些简单的问题，公司可以增强开发者的能力，并建立人工智能实现其全部潜力所需的道德开发基础设施。