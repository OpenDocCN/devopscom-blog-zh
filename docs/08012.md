# 分布式数据库实用指南

> 原文:[https://devo PS . com/no-废话-分布式数据库指南/](https://devops.com/no-nonsense-guide-to-distributed-databases/)

15 年前，我得到了我的第一份 DBA 工作，从那以后，很多事情都发生了变化。数据量爆炸式增长，我们看到了 NoSQL 的崛起，业务逻辑从存储在关系数据库中转移到了应用程序中。

直到今天，可靠性、可伸缩性和可维护性这三大支柱仍然是 DBA 的生存之本。更有趣的是，你每月为托管的[数据库](https://devops.com/?s=database)支付的云账单让你的首席技术官失去了生存的意愿。

因此，除了这三个支柱之外，现在您还有两个额外的事情要担心:成本效率和供应商不可知论。

在本文中，我将(试着)运行一个开源的新一代数据库，对其进行基准测试，并触及一些云中立的挑战。

**TL；博士**

*   在 Kubernetes 世界中解决 SQL 与 NoSQL 的困境
*   如何在多云世界中利用分布式 SQL 数据库
*   驯服性能—单一云和多云
*   将 TPC-C 作为事务基准运行
*   驯服成本——出口成本的问题

## 新时代的新包装

就在五年前，人们会说，“容器化无状态微服务？当然可以！没问题。”

但是当你真正容器化一个数据库时，你的持久层正在自找麻烦。

随着时间的推移，我们越来越喜欢集装箱。现在，它是软件打包和运输的默认方式。见鬼，甚至 AWS Lambda 也屈服了，开始运行容器映像。数据库无法抗拒。

一旦数据库被容器化——比方说 PostgreSQL 数据库——Kubernetes 就会接手处理可靠性和可维护性方面的一些更棘手的问题(在特定于中间件的操作人员的帮助下)。

容器将数据从运行时中分离出来，因此通过基础设施即代码(YAML)，升级和配置更加容易。Kubernetes 的内置功能涵盖了硬件故障和维护窗口，可以在幸存的节点上重新安排挂起的 pod。完整的定时备份或时间点恢复由操作员自动完成。

但是可伸缩性呢？

Kubernetes 的垂直 pod autoscaler 可以调整容器资源，使用新的资源设置删除和重新创建 pod。作为传统的关系数据库，PostgreSQL 只能有一个写数据的主副本。这意味着当新的 pod 上线时，您将面临短暂的停机时间。大多数工作负载的规模大约是五倍，因此尽管调整规模很痛苦，但总比完全不调整要好(请宽恕您的 CTO 的预算)。

NoSQL 没有这些限制。像 Cassandra 这样的数据库是为无停机水平扩展而构建的。

### 输入分布式 SQL

如果我们可以将 Google Spanner 的远距离操作能力与强酸一致性、Cassandra 的伸缩性和 PostgreSQL 兼容性相结合，会怎么样？

这就是 YugabyteDB 帮助的地方。

YugabyteDB 是一个开源的、关系型的、分布式的 SQL 数据库管理系统。它旨在跨多个可用性区域和地理区域处理大量数据，同时提供个位数延迟、高可用性和无单点故障。

我讨厌新来的人带来他们自己的语法语言，因为，你知道，他们的情况非常特别，就像其他人一样。

Yugabyte 不会这么做。他们有一个非常聪明的存储层(灵感来自 Google Spanner ),可以重用 PostgreSQL 查询引擎。这意味着开发团队没有精神压力——你可以使用久经考验的、可销售的和 [非常强大的 PostgreSQL 语言级特性](https://docs.yugabyte.com/latest/quick-start/explore/ysql/) ，许多人已经习惯了。

如果 Google Spanner 如此鼓舞人心，为什么它没有接管分布式 SQL 的世界？嗯，它只能作为 SaaS 使用，并且严重依赖于谷歌云硬件的细节，如原子钟(trueTime)和谷歌的主干网络。如果你的私人数据中心或最喜欢的云提供商不是 GCP——好吧，我的朋友，没有扳手给你。就我个人而言，我会犹豫是否要嫁入终极 [厂商锁定](https://cast.ai/blog/vendor-lock-in-and-how-to-break-free/) 。Yugabyte 没有任何依赖性，可以在任何云、私有数据中心、虚拟机或裸机上运行——如果您的咖啡机运行 Docker 容器，您也可以在那里运行 Yugabyte。

如果 Yugabyte 是健壮的、可伸缩的和易于维护的，那么你就拥有了最重要的三个支柱。但是你的 CTO 向你唠叨的另外两个支柱(成本效率和独立于供应商的方法)是什么呢？

### 如何使您的数据库独立于供应商

与供应商无关意味着能够在任何时候在主要的云提供商之间切换，而不依赖像 DynamoDB 这样昂贵的服务。这种方法还允许您利用云定价中的低效率，以便您可以寻找新的机会来降低成本，并在线在云提供商之间动态转移资源。

这是 [施展 AI](https://cast.ai) 才能做到的事情。

CAST AI 平台充当云抽象，并使整个多云、厂商无关的需求变得相当琐碎。

开发者只是和标准的 Kubernetes(没有 CRD)交互，几个云的复杂性就抽象出来了。CAST AI 中的一个 Kubernetes 集群可以同时跨越几个云提供商，带来一些独特的 [成本优化](https://cast.ai/blog/how-to-reduce-cloud-costs-by-90-spot-instances-and-how-to-use-them/) 机会。

如果**spot/preemptable**实例中断不再完全是随机的，而是你实际上可以预测的事情，会怎么样？您可以积极主动，通过同时在多个计算投标市场开展业务，降低容量耗尽的风险。

为什么不通过在现场/可抢占节点上运行一些数据库容器来大幅削减每月的云账单呢？

## 多云平台上的高可用性有状态应用

在多云 CAST AI Kubernetes 集群上的 Yugabyte 是一个健壮的设计，没有单点故障。

主动/被动解决方案太过时了，故障切换会导致短暂的停机，并且由于偶尔出现短暂的网络故障，数据库管理员倾向于让故障切换手动启动。主动/被动解决方案不断需要测试和验证。你的灾难恢复能在黑暗的日子里工作吗？因为这些测试会导致停机，所以没有人会定期进行测试。

最好的高可用性解决方案是主动-主动的，不需要用户故障转移，并且如果不定期测试也不会失效。

CAST AI 有一个 [教程](https://github.com/castai/examples/tree/main/boutique-eshop-yugabyte-dev) 带你在大约 10 分钟内从头开始完成构建上述解决方案的每一步。

继续，打开你的云提供商的控制台，通过杀死 Yugabyte、商业应用 pods 或整个 Kubernetes 节点来释放你内心的 [混沌猴](https://netflix.github.io/chaosmonkey/) 。

CAST AI、Yugabyte 和 Kubernetes 的核心宗旨是没有什么是有保证的；什么都不该分享。但请记住，当仍有多数人投票支持 N/2 + 1 时，共识算法是有效的。

您可能会问，当使用分布式数据库实现多云时，增加弹性会对性能产生什么影响？

让我们找出答案。

让我们在跨越 AWS 和 GCP 的 CAST AI Kubernetes 之上对分布式 Yugabyte 数据库进行基准测试。这些云提供商在这里是任意的选择；天蓝色或数字海洋也可以混合使用。我们选择了 GCP 和 AWS，因为它们的虚拟机启动速度更快。

## 单云和双云网络基准测试结果

让我们比较一下在单一云场景中 Kubernetes 上的 Yugabyte 容器化数据库和在两个不同云中工作的分布式数据库。

### 设置

我们用复制因子为 3 的镜像版本“2.5.1.0”拍摄了 OSS Yugabyte HELM 图表。

| **角色** | **YB-Master** | **y b-Tserver**T3】 |
| **描述** | 控制平面，共识 | 服务数据 |
| **CPU** | 3 个 CPU | 7 个 CPU |
| **公羊** | 3 个钩 | 14 磅 |
| **储存** | 50 磅 | 300 吉卜 |

远程网络块存储:

*   EBS gp2 (300 IOPs)
*   标准持久磁盘(R225/W450 IOps)

九个单元，每个单元位于单独的虚拟机上，以确保充足的资源:

**单色云**

区域性 GKE k8s 群集，9x GCE N2d-standard-8 分布在三个可用性区域。谷歌美国东部 4 区的休闲 VPC 网络。

**双云**

AWS EC2 3x c5a . 2x 大型

GCP GCE 6x n2d-标准-8

云提供商之间的网络，使用基于 Wireguard 的全网状 VPN 对流量进行端到端加密(也在同一个 VPC 内)。

为了最大限度地降低计算噪音和本地存储效率，我们没有针对成本进行优化，而是希望隔离和更好地了解分布式数据库在数据库节点分布在一位数毫秒(在本例中，不到 9 毫秒)的情况下如何工作。

九毫秒的网络往返延迟意味着*在数百英里内的数据中心提供商之间进行选择时有很大的灵活性。*

大多数云提供商地区缺乏想象力，并且聚集在人口稠密的大都市附近。加州、北弗吉尼亚、蒙特利尔、圣保罗、伦敦、法兰克福、阿姆斯特丹、孟买、东京、新加坡、悉尼等。

那么，你在法兰克福有什么选择？AWS、Azure、GCP、DigitalOcean、甲骨文、阿里巴巴、IBM Cloud、Hetzner、Linode——更不用说白牌供应商了。

我们在美国东部(阿什伯恩)和欧洲中部(法兰克福)进行了测试，结果相似。

### 我们如何测量它

如果我们想测试关系数据库的性能——迄今为止的黄金标准是合成的 TPC-C 基准。

我们为什么选择这个？因为它是最成熟的 OLTP 基准测试套件，经受住了时间的考验。如这里所说的[](http://www.tpc.org/):

“虽然该基准描述了批发供应商的活动，但 TPC-C 并不局限于任何特定业务部门的活动，而是代表了必须管理、销售或分销产品或服务的任何行业。”

我们在两个场景中加载了一个相同的 Yugabyte 配置，包含 300 个仓库的数据，然后运行了几次基准测试。

每个测试花费半个小时，输出是唯一最重要的度量:每分钟的事务。测试还提供了每种交易类型的不同输出，以毫秒为单位的平均持续时间和第 99 百分位持续时间。

### 结果

此表显示了 TPC-C 结果与同一地区(美国东部)的 Google 和 AWS + Google 上单个云的平均持续时间的比较。

| **300 个仓库** | **单色云** | **双云** |
| 每 1800 秒的新订单交易数 | One hundred and thirteen thousand and seventy-one | One hundred and thirteen thousand two hundred and forty-seven |
| TPM-C | Three thousand seven hundred and sixty-nine | Three thousand seven hundred and seventy-four |
| 效率 | 97.6% | 97.8% |
| 新订单延迟平均毫秒 | Five hundred and eighty point two | Four hundred and ninety-eight point eight |
| 支付延迟平均毫秒 | Thirty point eight | Forty-six point seven |
| 订单状态延迟平均毫秒 | Thirty-three point eight | Thirty-two point four |
| 传递延迟平均毫秒 | One hundred and twenty-three point one | One hundred and seventy-four point nine |
| 库存级别延迟平均毫秒 | Two hundred and thirty-nine point four | Four hundred and sixty-five |

比较第 99 百分位:

| **300 个仓库** | **单色云** | **双云** |
| 新顺序延迟p99 毫秒 | Six thousand nine hundred and eighty-five point six | Five thousand four hundred and ninety-eight point seven |
| 支付延迟 p99 毫秒 | One hundred and ten point five | One hundred and fifty-seven point five |
| 订单状态延迟 p99 毫秒 | One hundred and forty-six point three | One hundred and eighty-five point one |
| 交付延迟 p99 毫秒 | Three hundred and thirty-one point five | Six hundred and ninety-six point eight |
| 库存级延迟 p99 毫秒 | Seven hundred and twelve point three | One thousand and eighty-three point three |

如您所见，单云和双云场景之间的差异非常小。

有些操作，如 OrderStatus，是普通的读操作。最近的服务器可以用数据快速响应客户端——通常是运行在同一个 K8s 集群上的微服务。

Yugabyte 的最低隔离级别是快照。这意味着这些读取不会受到“脏读取”的不良影响

写作呢？

写操作的成本要高一些。Raft 共识、混合逻辑时钟和无锁多版本并发控制(MVCC)算法魔力——所有这些都意味着节点之间更多的喋喋不休和复制来获得写提交。

我们看到，在支付和交付操作持续时间方面(分别为 30 毫秒对 46 毫秒和 123 毫秒对 174 毫秒)，地理距离开始变得明显。

在没有网络覆盖和加密的 mono cloud 场景中，不同可用性区域中的节点之间的实际网络延迟平均约为 0.6 毫秒。具有覆盖(VXLAN)和加密(Wireguard)的两个云之间的网络延迟约为 5 毫秒。

物理定律不会很快消失。但是我们可以看到，结果比预期的要好很多。

YMMV，但业界公认的标准是，大多数 IO 操作将是读取，占 70%；只剩下 30%的写入，进一步降低了光速的影响。

话虽如此，过去二十年来衡量 OLTP 数据库性能的黄金标准表明性能结果是相同的(TPM-C 3'769 对 3'774)。

### 数据重力和出口成本

数据引力指的是计算向数据靠拢的情形。

例如，如果您有一个混合云或多云战略，但您的数据保存在一个位置，如本地数据中心。大多数具有无状态服务的计算将倾向于留在同一个数据中心，离数据更近。

多重云和高性能分布式数据库的全部意义在于消除数据重力。如果我们有两个或更多云的相同数据，那么数据重力效应就消失了。

如果您的所有数据都复制到几个位置，网络流量成本会怎么样？

所有云提供商都允许无限制的传入(进入)数据。数据来自哪里并不重要——是您的内部数据中心、不同的云提供商、黑暗网络还是大多数云原生服务。所有输入的数据都是免费的。

“你可以给我们带来尽可能多的数据。但如果你想拿回你的数据——对不起，你需要付费。”这是所有云提供商的共性。您的可用性区域的所有传出(出口)流量都是付费的！一些云提供商，如 Oracle Cloud，在出口成本方面非常慷慨，而 Zoom 等公司通过将 Zoom 的一些基础设施迁移到 Oracle CloudT3，节省了大量资金。

DigitalOcean 根据大小和运行时间的长短，每月为每个液滴提供大量的外出津贴。但是，至少，它是从每个最小的液滴万亿字节开始的。每月外出津贴会累计到您的账户中，您只需支付合计的额外费用。

所有云都有一些最小的免费层，但一般来说，VPC 以外发送的每 GB 数据的出口成本约为 10 美分，平均而言，同一 VPC 可用性区域之间的数据为 2 美分(我打赌你不知道你在 AWS 和 Azure 中为来自另一个 AZ 的同一 VPC/Vnet 的传入流量支付 1cnt/GB)。

如果你发送更多的流量，你的每 GB 成本下降。第一次下降是在 10TB 时，通常在大约 150TB 时持平在 5 美分左右。

当云提供商对出口流量收取令人眼红的 97%的利润时，你有什么感觉？这就像我们回到了 1995 年，用拨号调制解调器，对不对？

有一个性价比更高的替代方案。不过先来回答一下' [会融合吗？](https://www.youtube.com/watch?v=lAl28d6tbko) 疑问。让我们添加直连、快速路由、云互连以及第 2 层或第 3 层提供商，如 Equinix 或 Megaport。是的，你可以从搅拌机里拿出一杯专用的 Fiber Link 思慕雪。

专用光纤链路将成本削减至每 GB 仅 2 美分。如果您的出口流量超过 7TB，这比依赖云提供商的怪异折扣模式更具成本效益。

你可能会想，“每 GB 2 美分？！那太过分了！我正在使用 RDS—是的，我有供应商锁定和数据重力—但至少我没有出口成本、弹性、可扩展性和服务的总体可承受性方面的问题，对吗？”

这只是不透明的计费设计的结果。

![](../Images/1fc73d6314a394a452c2301331d044d4.png)

https://twitter.com/QuinnyPig/status/1372598432095891458

所有云中可用性区域之间的所有流量按每 GB 出口+入口 2 美分支付。唯一需要为入口付费的地方是同一地区的可用性区域之间(AWS/Azure)。一些托管服务提供免费复制，但在计算成本等方面收取双倍费用。

通过 ServiceTopology 功能，Kubernetes 可以将您的应用 pod 的流量保持在同一个云服务提供商可用性区域和云中，以避免云之间的循环、随机调用。这意味着您可以获得最佳的延迟，并且无需支付额外的流量费用。

大部分数据库活动是读取(70/30，YMMV)并且实际上是自由的，出口方式(由于 Kubernetes 拓扑感知，来自本地 AZ)。因此，您唯一需要复制并受制于出口成本的是您每月的数据库更改增量率。

现在你可以看到出口成本是一个被过分夸大的问题。

过去十年对公共云的大力推动带来了新的挑战，如供应商锁定、数据重力和令人垂涎的每月基础架构账单。

在此期间，非传统 RDBMS、集装箱化和工作负载调度方面的创新消除了一些旧的障碍。

十年前，在预算有限的情况下，在异构数据中心运行一个无需人工干预、高度一致、高度可用且可扩展的数据库是一项艰巨的任务。今天，它已经成为运行我们的业务应用程序的一种可承受的最佳实践。